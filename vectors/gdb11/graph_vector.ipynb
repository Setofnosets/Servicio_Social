{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import os\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear grafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gdb11_size01.smi', 'gdb11_size02.smi', 'gdb11_size03.smi', 'gdb11_size04.smi', 'gdb11_size05.smi', 'gdb11_size06.smi', 'gdb11_size07.smi', 'gdb11_size08.smi', 'gdb11_size09.smi', 'gdb11_size10.smi', 'gdb11_size11.smi']\n"
     ]
    }
   ],
   "source": [
    "#Leer archivos\n",
    "archivos = os.listdir('..\\\\gdb11')\n",
    "print(archivos)\n",
    "\n",
    "#Crear carpeta para guardar grafos\n",
    "\"\"\"if not os.path.exists('grafos'):\n",
    "    os.makedirs('grafos')\"\"\"\n",
    "\n",
    "data = []\n",
    "#Crear grafos\n",
    "for archivo in archivos:\n",
    "    #if archivo == 'gdb11_size10.smi':\n",
    "        #break\n",
    "    with open('..\\\\gdb11\\\\'+archivo, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        #Crear grafo\n",
    "        for line in lines:\n",
    "            mol = Chem.MolFromSmiles(line.split()[0])\n",
    "            smile = line.split()[0]\n",
    "            adj = Chem.GetAdjacencyMatrix(mol)\n",
    "            nodesym = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "            bond_type = [bond.GetBondType() for bond in mol.GetBonds()]\n",
    "            G = nx.Graph(adj, symbol=nodesym, bond_type=bond_type, smile=smile)\n",
    "            pos = nx.spring_layout(G)\n",
    "\n",
    "            data.append(nx.node_link_data(G))\n",
    "            #nx.draw(G, pos, labels={i: G.graph['symbol'][i] for i in range(len(G.graph['symbol']))}, with_labels=True)\"\"\"\n",
    "\n",
    "#Guardar json\n",
    "import json\n",
    "with open('grafos.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[5, 1], edge_index=[2, 4], edge_attr=[4, 1])\n",
      "tensor([[67.],\n",
      "        [78.],\n",
      "        [67.],\n",
      "        [78.],\n",
      "        [79.]])\n",
      "tensor([[0, 1, 2, 2],\n",
      "        [1, 2, 3, 4]])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "523902\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "with open('grafos.json', 'r') as f:\n",
    "    graph_data = json.load(f)\n",
    "\n",
    "\n",
    "def atom_features(atom):\n",
    "    return [\n",
    "        atom.GetAtomicNum(),  # Atomic number\n",
    "        atom.GetDegree(),      # Number of bonded neighbors\n",
    "        atom.GetHybridization(),\n",
    "        atom.GetIsAromatic()\n",
    "    ]\n",
    "\n",
    "# No supervisado\n",
    "def nx_to_pyg(graph):\n",
    "    # Extract node features (symbols)\n",
    "    node_features = torch.tensor([[ord(symbol)] for symbol in graph.graph['symbol']], dtype=torch.float)\n",
    "    #node_features = torch.tensor([atom_features(atom) for atom in Chem.MolFromSmiles(graph.graph['smile']).GetAtoms()], dtype=torch.float)\n",
    "    # Extract edge indices\n",
    "    edge_index = torch.tensor(list(graph.edges), dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Extract edge features (bond types)\n",
    "    edge_attr = torch.tensor([bond_type for bond_type in graph.graph['bond_type']], dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# Convert all graphs to PyG format\n",
    "pyg_data_list = [nx_to_pyg(nx.node_link_graph(graph)) for graph in graph_data]\n",
    "print(pyg_data_list[200])\n",
    "print(pyg_data_list[200].x)\n",
    "print(pyg_data_list[200].edge_index)\n",
    "print(pyg_data_list[200].edge_attr)\n",
    "print(len(pyg_data_list))\n",
    "\n",
    "# Show the first graph\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "#G = to_networkx(pyg_data_list[200])\n",
    "#print(G)\n",
    "#pos = nx.spring_layout(G)\n",
    "#nx.draw(G, pos, labels={i: chr(int(pyg_data_list[200].x[i])) for i in range(len(G))}, with_labels=True)\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class CustomGraphDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(CustomGraphDataset, self).__init__()\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomGraphDataset(pyg_data_list)\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Save the dataset\n",
    "torch.save(dataset, 'dataset.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (initial_conv): GCNConv(1, 64)\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=128, out_features=0, bias=True)\n",
      ")\n",
      "Number of parameters:  12608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\locua\\anaconda3\\Lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "embedding_size = 64\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Init\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        # GCN layers\n",
    "        self.initial_conv = GCNConv(dataset.num_node_features, embedding_size)\n",
    "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
    "        # Output layer\n",
    "        self.out = Linear(embedding_size*2, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Others\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Global Pooling (stack different aggregations)\n",
    "        hidden = torch.cat([gmp(hidden, batch_index), gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "        # Final\n",
    "        out = self.out(hidden)\n",
    "        return out, hidden\n",
    "    \n",
    "model = GCN()\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1638\n",
      "Comienzo del entrenamiento\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n",
      "Epoch 40\n",
      "Epoch 41\n",
      "Epoch 42\n",
      "Epoch 43\n",
      "Epoch 44\n",
      "Epoch 45\n",
      "Epoch 46\n",
      "Epoch 47\n",
      "Epoch 48\n",
      "Epoch 49\n",
      "Epoch 50\n",
      "Epoch 51\n",
      "Epoch 52\n",
      "Epoch 53\n",
      "Epoch 54\n",
      "Epoch 55\n",
      "Epoch 56\n",
      "Epoch 57\n",
      "Epoch 58\n",
      "Epoch 59\n",
      "Epoch 60\n",
      "Epoch 61\n",
      "Epoch 62\n",
      "Epoch 63\n",
      "Epoch 64\n",
      "Epoch 65\n",
      "Epoch 66\n",
      "Epoch 67\n",
      "Epoch 68\n",
      "Epoch 69\n",
      "Epoch 70\n",
      "Epoch 71\n",
      "Epoch 72\n",
      "Epoch 73\n",
      "Epoch 74\n",
      "Epoch 75\n",
      "Epoch 76\n",
      "Epoch 77\n",
      "Epoch 78\n",
      "Epoch 79\n",
      "Epoch 80\n",
      "Epoch 81\n",
      "Epoch 82\n",
      "Epoch 83\n",
      "Epoch 84\n",
      "Epoch 85\n",
      "Epoch 86\n",
      "Epoch 87\n",
      "Epoch 88\n",
      "Epoch 89\n",
      "Epoch 90\n",
      "Epoch 91\n",
      "Epoch 92\n",
      "Epoch 93\n",
      "Epoch 94\n",
      "Epoch 95\n",
      "Epoch 96\n",
      "Epoch 97\n",
      "Epoch 98\n",
      "Epoch 99\n",
      "[[ 0.5106915  -0.45644715 -0.50678694 ... -0.36097744 -0.16058762\n",
      "  -0.0743839 ]\n",
      " [ 0.502553   -0.48920816 -0.5399986  ... -0.36212865 -0.16600168\n",
      "  -0.08009386]\n",
      " [ 0.5106915  -0.49164525 -0.5399986  ... -0.3627006  -0.16073062\n",
      "  -0.07731713]\n",
      " ...\n",
      " [ 0.51038414 -0.49092564 -0.5399986  ... -0.35611266 -0.16023152\n",
      "  -0.07798667]\n",
      " [ 0.502553   -0.48920816 -0.5400011  ... -0.3565357  -0.16549198\n",
      "  -0.07920091]\n",
      " [ 0.5106915  -0.4566683  -0.5067954  ... -0.3579366  -0.1603986\n",
      "  -0.07642024]]\n",
      "(49, 128)\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "data_size = len(dataset)\n",
    "batch_size = 256\n",
    "\n",
    "loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=batch_size, shuffle=True)\n",
    "print(len(loader))\n",
    "test_loader = DataLoader(dataset[int(data_size * 0.8):], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def train(data):\n",
    "    for batch in loader:\n",
    "        batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred, embedding = model(batch.x, batch.edge_index, batch.batch)\n",
    "        optimizer.step()\n",
    "    return embedding\n",
    "\n",
    "print(\"Comienzo del entrenamiento\")\n",
    "losses = []\n",
    "embeddings = []\n",
    "for epoch in range(100):\n",
    "    embedding = train(dataset)\n",
    "    #losses.append(loss)\n",
    "    embeddings.append(embedding)\n",
    "    print(f'Epoch {epoch}')\n",
    "\n",
    "# Visualize the embeddings\n",
    "# The embeddings are the output of the global pooling layer\n",
    "# The embeddings are the concatenation of the global max pooling and global mean pooling\n",
    "embedding = embeddings[-1].detach().cpu().numpy()\n",
    "print(embedding)\n",
    "print(embedding.shape)\n",
    "print(len(embedding))\n",
    "\n",
    "def embedding_to_csv(embeddings, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for embedding in embeddings:\n",
    "            f.write(','.join([str(x) for x in embedding.tolist()]) + '\\n')\n",
    "    \n",
    "embedding_to_csv(embeddings, 'embeddings.csv')\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar Con Perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 1, Loss: 3.394988247490791\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m train_contrastive(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     66\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n",
      "Cell \u001b[1;32mIn[13], line 47\u001b[0m, in \u001b[0;36mtrain_contrastive\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m     45\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 47\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model(batch\u001b[38;5;241m.\u001b[39mx, batch\u001b[38;5;241m.\u001b[39medge_index, batch\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m contrastive_loss(embeddings)\n\u001b[0;32m     49\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\locua\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\locua\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m, in \u001b[0;36mContrastiveGNN.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch):\n\u001b[1;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index)\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[1;32mc:\\Users\\locua\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\locua\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\locua\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:362\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    357\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_updater(edge_index, alpha\u001b[38;5;241m=\u001b[39malpha, edge_attr\u001b[38;5;241m=\u001b[39medge_attr,\n\u001b[0;32m    363\u001b[0m                           size\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, alpha\u001b[38;5;241m=\u001b[39malpha, size\u001b[38;5;241m=\u001b[39msize)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gat_conv_GATConv_edge_updater_ikqdcf5m.py:176\u001b[0m, in \u001b[0;36medge_updater\u001b[1;34m(self, edge_index, alpha, edge_attr, size)\u001b[0m\n\u001b[0;32m    166\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[0;32m    167\u001b[0m                 alpha_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    168\u001b[0m                 alpha_i\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha_i\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    173\u001b[0m             )\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# End Edge Update Forward Pre Hook #########################################\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_update(\n\u001b[0;32m    177\u001b[0m     alpha_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39malpha_j,\n\u001b[0;32m    178\u001b[0m     alpha_i\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39malpha_i,\n\u001b[0;32m    179\u001b[0m     edge_attr\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39medge_attr,\n\u001b[0;32m    180\u001b[0m     index\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mindex,\n\u001b[0;32m    181\u001b[0m     ptr\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mptr,\n\u001b[0;32m    182\u001b[0m     dim_size\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mdim_size,\n\u001b[0;32m    183\u001b[0m )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Begin Edge Update Forward Hook ###########################################\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[1;32mc:\\Users\\locua\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:409\u001b[0m, in \u001b[0;36mGATConv.edge_update\u001b[1;34m(self, alpha_j, alpha_i, edge_attr, index, ptr, dim_size)\u001b[0m\n\u001b[0;32m    406\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m+\u001b[39m alpha_edge\n\u001b[0;32m    408\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(alpha, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slope)\n\u001b[1;32m--> 409\u001b[0m alpha \u001b[38;5;241m=\u001b[39m softmax(alpha, index, ptr, dim_size)\n\u001b[0;32m    410\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(alpha, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m alpha\n",
      "File \u001b[1;32mc:\\Users\\locua\\anaconda3\\Lib\\site-packages\\torch_geometric\\utils\\_softmax.py:79\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(src, index, ptr, num_nodes, dim)\u001b[0m\n\u001b[0;32m     77\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m     78\u001b[0m     out_sum \u001b[38;5;241m=\u001b[39m scatter(out, index, dim, dim_size\u001b[38;5;241m=\u001b[39mN, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-16\u001b[39m\n\u001b[1;32m---> 79\u001b[0m     out_sum \u001b[38;5;241m=\u001b[39m out_sum\u001b[38;5;241m.\u001b[39mindex_select(dim, index)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "class ContrastiveGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(ContrastiveGNN, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GATConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GATConv(hidden_dim, hidden_dim)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Contrastive loss function\n",
    "def contrastive_loss(embeddings, temperature=0.1):\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "    \n",
    "    # Contrastive loss (example: NT-Xent)\n",
    "    # Replace this with your actual contrastive loss implementation\n",
    "    loss = -torch.log(torch.diag(sim_matrix).exp() / sim_matrix.exp().sum(dim=1))\n",
    "    return loss.mean()\n",
    "\n",
    "# Training loop for contrastive learning\n",
    "def train_contrastive(epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = contrastive_loss(embeddings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Initialize the model\n",
    "model = ContrastiveGNN(input_dim=dataset.num_node_features, hidden_dim=128, embedding_dim=64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_contrastive(epochs=100)\n",
    "\n",
    "embeddings = []\n",
    "for batch in dataloader:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings.append(model(batch.x, batch.edge_index, batch.batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featurizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5106915  -0.49164522 -0.5399986  ... -0.3614224  -0.16016841\n",
      "  -0.07859194]\n",
      " [ 0.5106915  -0.45544335 -0.50530016 ... -0.35572392 -0.15966657\n",
      "  -0.07829054]\n",
      " [ 0.5297204  -0.49081612 -0.5399988  ... -0.37471935 -0.16792496\n",
      "  -0.07916652]\n",
      " ...\n",
      " [ 0.5106915  -0.48980203 -0.5399988  ... -0.35558358 -0.1618756\n",
      "  -0.07869917]\n",
      " [ 0.5106915  -0.49159074 -0.5399986  ... -0.36136273 -0.16039062\n",
      "  -0.07865348]\n",
      " [ 0.5106915  -0.4914373  -0.5399986  ... -0.3569494  -0.15863296\n",
      "  -0.07605761]]\n",
      "(17, 128)\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "embedding = embeddings[-1].detach().cpu().numpy()\n",
    "print(embedding)\n",
    "print(embedding.shape)\n",
    "print(len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5106915  -0.4920931  -0.5399988  -0.23681097  0.69906086  0.378033\n",
      "   0.6728528  -0.3410698  -0.02140541 -0.3838586  -0.16989408 -0.48199406\n",
      "   0.59261876  0.39275134  0.82839155  0.46965823  0.70105994 -0.03312314\n",
      "   0.6600193  -0.53970134  0.5308237  -0.6937653  -0.3765128  -0.15563856\n",
      "  -0.24933942  0.12511554  0.09804676  0.68223536  0.47348642  0.72101986\n",
      "  -0.19723928  0.10990268 -0.5461353   0.7248185   0.35445288 -0.22667696\n",
      "   0.50009793  0.22784697 -0.03144305 -0.11254678  0.40345314  0.8560601\n",
      "  -0.2608003   0.43231446  0.72672033 -0.73081625  0.7488088  -0.3922557\n",
      "  -0.20873761 -0.38060647 -0.425526   -0.01969332  0.6490324   0.75536466\n",
      "  -0.7369295   0.4101855   0.9153064   0.23333922 -0.37043187 -0.3937288\n",
      "   0.44654912 -0.34494868 -0.15267359 -0.06420745  0.45360258 -0.5310152\n",
      "  -0.5906396  -0.2729106   0.6348447   0.33583236  0.6066788  -0.3874403\n",
      "  -0.02549218 -0.41605377 -0.2046188  -0.5271914   0.53462243  0.34657347\n",
      "   0.77175945  0.41269475  0.63477343 -0.03906127  0.5985642  -0.5910617\n",
      "   0.47317603 -0.73525614 -0.4174716  -0.186689   -0.2771216   0.1120533\n",
      "   0.08184891  0.61898005  0.41852373  0.65770173 -0.2242112   0.09659762\n",
      "  -0.59488654  0.6620507   0.31342307 -0.2506038   0.4390419   0.19609296\n",
      "  -0.04596766 -0.12848362  0.3554169   0.80340433 -0.27921715  0.38382322\n",
      "   0.6615356  -0.77709997  0.6858389  -0.42735314 -0.22629201 -0.41491866\n",
      "  -0.4651121  -0.02297332  0.5849005   0.6923367  -0.77857196  0.35572657\n",
      "   0.8723315   0.20540306 -0.40489024 -0.44296086  0.39332652 -0.37262684\n",
      "  -0.16346297 -0.07269321]]\n",
      "(1, 128)\n",
      "[[ 0.5106915  -0.4920931  -0.5399988  -0.23681097  0.69906086  0.378033\n",
      "   0.6728528  -0.3410698  -0.02116291 -0.3838586  -0.16989408 -0.48199406\n",
      "   0.59261876  0.39275134  0.82839155  0.46965823  0.70105994 -0.03312314\n",
      "   0.6600193  -0.53970134  0.5308237  -0.6937653  -0.3765128  -0.15563856\n",
      "  -0.24933942  0.12511554  0.09804676  0.68223536  0.47348642  0.72101986\n",
      "  -0.19723928  0.10990268 -0.5461353   0.7248185   0.35445288 -0.22667696\n",
      "   0.50009793  0.22784697 -0.03144305 -0.11254678  0.40345314  0.8560601\n",
      "  -0.2608003   0.43231446  0.72672033 -0.73081625  0.7488088  -0.3922557\n",
      "  -0.20807678 -0.38060647 -0.425526   -0.01921003  0.6490324   0.75536466\n",
      "  -0.7369295   0.4101855   0.9153064   0.23333922 -0.37043187 -0.3937288\n",
      "   0.44654912 -0.34494868 -0.15267359 -0.06420745  0.4534848  -0.53096634\n",
      "  -0.590641   -0.27294058  0.6347536   0.33596492  0.60656166 -0.38746244\n",
      "  -0.02543155 -0.41616133 -0.20465018 -0.52720714  0.5347181   0.34658745\n",
      "   0.77179015  0.41252002  0.6349595  -0.03914909  0.5985049  -0.5910796\n",
      "   0.4731705  -0.73520696 -0.41764852 -0.1868836  -0.27715862  0.11189562\n",
      "   0.08164789  0.6188551   0.41846934  0.6577171  -0.22412413  0.09655544\n",
      "  -0.59502095  0.6619897   0.31356162 -0.25078288  0.43908238  0.1960372\n",
      "  -0.04600287 -0.12861556  0.35514918  0.8034098  -0.27925077  0.3836502\n",
      "   0.6616263  -0.777118    0.6857614  -0.42741898 -0.2261268  -0.41501492\n",
      "  -0.4651922  -0.0228525   0.5847937   0.69243157 -0.77861893  0.35549837\n",
      "   0.87232697  0.20536838 -0.4049605  -0.44308195  0.3932637  -0.37257048\n",
      "  -0.16365539 -0.07274766]]\n",
      "(1, 128)\n",
      "0.0012142978375777602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\locua\\anaconda3\\Lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "C:\\Users\\locua\\AppData\\Local\\Temp\\ipykernel_9604\\2422508820.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo\n",
    "model = GCN()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Cargar los embeddings\n",
    "#embedding = np.loadtxt('embeddings.csv', delimiter=',')\n",
    "\n",
    "# Sentence2vec\n",
    "mol = Chem.MolFromSmiles('CC(C)C')\n",
    "adj = Chem.GetAdjacencyMatrix(mol)\n",
    "\n",
    "nodesym = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "bond_type = [bond.GetBondType() for bond in mol.GetBonds()]\n",
    "G = nx.Graph(adj, symbol=nodesym, bond_type=bond_type)\n",
    "\n",
    "def get_vector(G):\n",
    "    data = nx_to_pyg(G)\n",
    "    data.to(device)\n",
    "    _, embedding = model(data.x, data.edge_index, data.batch)\n",
    "    return embedding.detach().cpu().numpy()\n",
    "\n",
    "vector = get_vector(G)\n",
    "print(vector)\n",
    "print(vector.shape)\n",
    "\n",
    "# Calcular la distancia euclidiana\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "mol2 = Chem.MolFromSmiles('CC(C)O')\n",
    "adj2 = Chem.GetAdjacencyMatrix(mol2)\n",
    "\n",
    "nodesym2 = [atom.GetSymbol() for atom in mol2.GetAtoms()]\n",
    "bond_type2 = [bond.GetBondType() for bond in mol2.GetBonds()]\n",
    "G2 = nx.Graph(adj2, symbol=nodesym2, bond_type=bond_type2)\n",
    "\n",
    "vector2 = get_vector(G2)\n",
    "print(vector2)\n",
    "print(vector2.shape)\n",
    "\n",
    "print(euclidean(vector.flatten(), vector2.flatten()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
